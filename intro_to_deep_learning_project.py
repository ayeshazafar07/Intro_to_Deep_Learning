# -*- coding: utf-8 -*-
"""Intro_to_Deep_Learning_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11T8NXWU4mfUo21a6nbnEoLb2SjQOtAjt

<div class="alert alert-block alert-warning">
    <h1><center> Introduction to Deep Learning - Project  </center></h1>

# Importing required libraries
"""

import numpy as np
from tensorflow.keras.utils import to_categorical
import tensorflow as tf
from tensorflow.keras import layers, models, regularizers
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import LeakyReLU
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adagrad
import tensorflow_datasets as tfds
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

"""# Data Loading, Preparation, and Preprocessing"""

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar100.load_data()
CIFAR100_LABELS = [
    'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle', 'bowl', 'boy',
    'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock',
    'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 'dolphin', 'elephant', 'flatfish',
    'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',
    'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', 'oak_tree', 'orange',
    'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',
    'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark', 'shrew', 'skunk',
    'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table',
    'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe',
    'whale', 'willow_tree', 'wolf', 'woman', 'worm'
]

# To map class names to indices
def map_classes_to_indices(selected_classes, class_labels):
    return [class_labels.index(cls) for cls in selected_classes]

# To extract data for selected classes
def extract_data_for_classes(x_data, y_data, selected_class_indices):
    selected_indices = np.isin(y_data, selected_class_indices).flatten()
    x_selected = x_data[selected_indices]
    y_selected = y_data[selected_indices]

    return x_selected, y_selected

# To map original labels to new class indices
def remap_labels(y_data, selected_class_indices):
    class_mapping = {old_label: new_label for new_label, old_label in enumerate(selected_class_indices)}
    return np.vectorize(class_mapping.get)(y_data)

# To reduce dataset size by selecting a fixed number of samples per class
def reduce_dataset_size(x_data, y_data, num_images_per_class):
    x_final, y_final = [], []
    for cls in range(len(selected_classes)):
        cls_indices = np.where(y_data == cls)[0][:num_images_per_class]
        x_final.append(x_data[cls_indices])
        y_final.append(y_data[cls_indices])
    x_final = np.concatenate(x_final, axis=0)
    y_final = np.concatenate(y_final, axis=0)

    return x_final, y_final

# To preprocess data
def preprocess_data(x_data, y_data, num_classes):
    x_data = x_data.astype('float32') / 255.0
    y_data = to_categorical(y_data, num_classes=num_classes)

    return x_data, y_data

# We can either select classes like this or randomly choose
# selected_classes = ['skyscraper', 'motorcycle', 'poppy', 'squirrel', 'dinosaur', 'worm', 'plain', 'tractor', 'keyboard', 'rocket']

# 10 random classes from CIFAR-100
np.random.seed(62)  # To ensure reproducibility
selected_classes = np.random.choice(CIFAR100_LABELS, 10, replace=False)
print(f"Selected classes: {selected_classes}")

selected_class_indices = map_classes_to_indices(selected_classes, CIFAR100_LABELS)

x_train_selected, y_train_selected = extract_data_for_classes(x_train, y_train, selected_class_indices)
x_test_selected, y_test_selected = extract_data_for_classes(x_test, y_test, selected_class_indices)

y_train_selected = remap_labels(y_train_selected, selected_class_indices)
y_test_selected = remap_labels(y_test_selected, selected_class_indices)

x_train_final, y_train_final = reduce_dataset_size(x_train_selected, y_train_selected, 300)
x_test_final, y_test_final = reduce_dataset_size(x_test_selected, y_test_selected, 50)

x_train_final, y_train_final = preprocess_data(x_train_final, y_train_final, len(selected_classes))
x_test_final, y_test_final = preprocess_data(x_test_final, y_test_final, len(selected_classes))

print(f"Train Data Shape: {x_train_final.shape}, Train Labels: {y_train_final.shape}")
print(f"Test Data Shape: {x_test_final.shape}, Test Labels: {y_test_final.shape}")

"""# Building and Evaluating Base CNN Model"""

def build_cnn_model(input_shape=(32, 32, 3), num_classes=10):
    model = models.Sequential()
    model.add(layers.Input(shape=input_shape))

    model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu'))
    model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu'))
    model.add(layers.MaxPooling2D(pool_size=(2, 2)))
    model.add(layers.Dropout(0.25))

    model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))
    model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))
    model.add(layers.MaxPooling2D(pool_size=(2, 2)))
    model.add(layers.Dropout(0.25))

    model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))
    model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))
    model.add(layers.MaxPooling2D(pool_size=(2, 2)))
    model.add(layers.Dropout(0.25))

    model.add(layers.Flatten())
    model.add(layers.Dense(512, activation='relu'))
    model.add(layers.Dropout(0.5))
    model.add(layers.Dense(num_classes, activation='softmax'))

    model.compile(
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    return model

cnn_model = build_cnn_model()
cnn_model.summary()

history = cnn_model.fit(x_train_final, y_train_final,
                    epochs=25,
                    batch_size=64,
                    validation_data=(x_test_final, y_test_final))

test_loss, test_accuracy = cnn_model.evaluate(x_test_final, y_test_final)
print(f"Accuracy: {test_accuracy * 100:.2f}%")

plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training vs Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

print("")
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training vs Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

"""# Experiments to Improve Accuracy (part 5)

## 1. Increasing the size and depth of the inner layers
"""

def build_deeper_model(input_shape=(32, 32, 3), num_classes=10):
    model = models.Sequential()
    model.add(layers.Input(shape=input_shape))

    model.add(layers.Conv2D(64, (3, 3), padding='same'))
    model.add(layers.BatchNormalization())
    model.add(layers.Activation('relu'))
    model.add(layers.Conv2D(64, (3, 3), padding='same'))
    model.add(layers.BatchNormalization())
    model.add(layers.Activation('relu'))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Dropout(0.3))

    model.add(layers.Conv2D(128, (3, 3), padding='same'))
    model.add(layers.BatchNormalization())
    model.add(layers.Activation('relu'))
    model.add(layers.Conv2D(128, (3, 3), padding='same'))
    model.add(layers.BatchNormalization())
    model.add(layers.Activation('relu'))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Dropout(0.4))

    model.add(layers.Conv2D(256, (3, 3), padding='same'))
    model.add(layers.BatchNormalization())
    model.add(layers.Activation('relu'))
    model.add(layers.Conv2D(256, (3, 3), padding='same'))
    model.add(layers.BatchNormalization())
    model.add(layers.Activation('relu'))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Dropout(0.4))

    model.add(layers.Conv2D(512, (3, 3), padding='same'))
    model.add(layers.BatchNormalization())
    model.add(layers.Activation('relu'))
    model.add(layers.Conv2D(512, (3, 3), padding='same'))
    model.add(layers.BatchNormalization())
    model.add(layers.Activation('relu'))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Dropout(0.5))

    model.add(layers.Flatten())
    model.add(layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)))
    model.add(layers.BatchNormalization())
    model.add(layers.Dropout(0.4))

    model.add(layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)))
    model.add(layers.BatchNormalization())
    model.add(layers.Dropout(0.3))
    model.add(layers.Dense(num_classes, activation='softmax'))

    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

deeper_model = build_deeper_model()
deeper_history = deeper_model.fit(x_train_final, y_train_final, epochs=25, batch_size=64, validation_data=(x_test_final, y_test_final))

deeper_test_loss, deeper_test_accuracy = deeper_model.evaluate(x_test_final, y_test_final)
print(f"Accuracy after optimizations: {deeper_test_accuracy * 100:.2f}%")

plt.plot(deeper_history.history['accuracy'], label='Train Accuracy')
plt.plot(deeper_history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training vs Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

print("")
plt.plot(deeper_history.history['loss'], label='Train Loss')
plt.plot(deeper_history.history['val_loss'], label='Validation Loss')
plt.title('Training vs Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

"""### Trying fine tuning on deeper model with data augmentation to check results"""

datagen = ImageDataGenerator(
    rotation_range=10,
    width_shift_range=0.05,
    height_shift_range=0.05,
    horizontal_flip=True,
    zoom_range=0.05
)
datagen.fit(x_train_final)

# Training without augmentation
baseline_model = build_deeper_model()
baseline_model.fit(
    x_train_final, y_train_final,
    epochs=20,
    validation_data=(x_test_final, y_test_final)
)

# Fine tuning with augmentation
aug_history = baseline_model.fit(
    datagen.flow(x_train_final, y_train_final, batch_size=64),
    epochs=50,
    validation_data=(x_test_final, y_test_final)
)

aug_test_loss, aug_test_accuracy = baseline_model.evaluate(x_test_final, y_test_final)
print(f"Accuracy after fine tuning: {aug_test_accuracy * 100:.2f}%")

plt.plot(aug_history.history['accuracy'], label='Train Accuracy')
plt.plot(aug_history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training vs Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

print("")
plt.plot(aug_history.history['loss'], label='Train Loss')
plt.plot(aug_history.history['val_loss'], label='Validation Loss')
plt.title('Training vs Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

"""## 2. Using fewer or more convolutional/maxpooling layers and different shapes"""

def build_simpler_model(input_shape=(32, 32, 3), num_classes=10):
    model = models.Sequential()
    model.add(layers.Input(shape=input_shape))

    model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu'))
    model.add(layers.MaxPooling2D((2, 2)))

    model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))
    model.add(layers.MaxPooling2D((2, 2)))

    model.add(layers.Flatten())
    model.add(layers.Dense(128, activation='relu'))
    model.add(layers.Dropout(0.3))
    model.add(layers.Dense(num_classes, activation='softmax'))

    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

simpler_model = build_simpler_model()
history_simple = simpler_model.fit(x_train_final, y_train_final,
                                  epochs=25,
                                  batch_size=64,
                                  validation_data=(x_test_final, y_test_final))

simpler_test_loss, simpler_test_accuracy = simpler_model.evaluate(x_test_final, y_test_final)
print(f"Accuracy (simpler): {simpler_test_accuracy * 100:.2f}%")

def build_deep_model(input_shape=(32, 32, 3), num_classes=10):
    model = models.Sequential()
    model.add(layers.Input(shape=input_shape))

    model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))
    model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))
    model.add(layers.MaxPooling2D((2, 2)))

    model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))
    model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))
    model.add(layers.MaxPooling2D((2, 2)))

    model.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))
    model.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))
    model.add(layers.MaxPooling2D((2, 2)))

    model.add(layers.Flatten())
    model.add(layers.Dense(256, activation='relu'))
    model.add(layers.Dropout(0.3))
    model.add(layers.Dense(num_classes, activation='softmax'))

    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

deep_model = build_deep_model()
history_deep = deep_model.fit(x_train_final, y_train_final,
                                  epochs=25,
                                  batch_size=64,
                                  validation_data=(x_test_final, y_test_final))

deep_test_loss, deep_test_accuracy = deep_model.evaluate(x_test_final, y_test_final)
print(f"Accuracy (deep): {deep_test_accuracy * 100:.2f}%")

"""## 3. Using different activation functions in the inner layers and in the convolutional layers"""

def cnn_with_tanh(input_shape=(32, 32, 3), num_classes=10):
    model = models.Sequential()
    model.add(layers.Input(shape=input_shape))
    model.add(layers.Conv2D(32, (3, 3), padding='same', activation='tanh'))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Conv2D(64, (3, 3), padding='same', activation='tanh'))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Conv2D(128, (3, 3), padding='same', activation='tanh'))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Flatten())
    model.add(layers.Dense(256, activation='tanh'))
    model.add(layers.Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

tanh_model = cnn_with_tanh()
tanh_history = tanh_model.fit(x_train_final, y_train_final,
                                  epochs=25,
                                  batch_size=64,
                                  validation_data=(x_test_final, y_test_final))

tanh_test_loss, tanh_test_accuracy = tanh_model.evaluate(x_test_final, y_test_final)
print(f"Accuracy (tanh): {tanh_test_accuracy * 100:.2f}%")

def cnn_with_sigmoid(input_shape=(32, 32, 3), num_classes=10):
    model = models.Sequential()
    model.add(layers.Input(shape=input_shape))
    model.add(layers.Conv2D(32, (3, 3), padding='same', activation='sigmoid'))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Conv2D(64, (3, 3), padding='same', activation='sigmoid'))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Conv2D(128, (3, 3), padding='same', activation='sigmoid'))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Flatten())
    model.add(layers.Dense(256, activation='sigmoid'))
    model.add(layers.Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

sigmoid_model = cnn_with_sigmoid()
sigmoid_history = sigmoid_model.fit(x_train_final, y_train_final,
                                  epochs=25,
                                  batch_size=64,
                                  validation_data=(x_test_final, y_test_final))

sigmoid_test_loss, sigmoid_test_accuracy = sigmoid_model.evaluate(x_test_final, y_test_final)
print(f"Accuracy (sigmoid): {sigmoid_test_accuracy * 100:.2f}%")

def cnn_with_leaky_relu(input_shape=(32, 32, 3), num_classes=10):
    model = models.Sequential()
    model.add(layers.Input(shape=input_shape))
    model.add(layers.Conv2D(32, (3, 3), padding='same'))
    model.add(LeakyReLU(alpha=0.1))
    model.add(layers.Conv2D(32, (3, 3), padding='same'))
    model.add(LeakyReLU(alpha=0.1))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Dropout(0.25))
    model.add(layers.Conv2D(64, (3, 3), padding='same'))
    model.add(LeakyReLU(alpha=0.1))
    model.add(layers.Conv2D(64, (3, 3), padding='same'))
    model.add(LeakyReLU(alpha=0.1))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Dropout(0.25))
    model.add(layers.Conv2D(128, (3, 3), padding='same'))
    model.add(LeakyReLU(alpha=0.1))
    model.add(layers.Conv2D(128, (3, 3), padding='same'))
    model.add(LeakyReLU(alpha=0.1))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Dropout(0.25))
    model.add(layers.Flatten())
    model.add(layers.Dense(256))
    model.add(LeakyReLU(alpha=0.1))
    model.add(layers.Dropout(0.3))
    model.add(layers.Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

leaky_relu_model = cnn_with_leaky_relu()
leaky_relu_history = leaky_relu_model.fit(x_train_final, y_train_final,
                                  epochs=25,
                                  batch_size=64,
                                  validation_data=(x_test_final, y_test_final))

leaky_relu_test_loss, leaky_relu_test_accuracy = leaky_relu_model.evaluate(x_test_final, y_test_final)
print(f"Accuracy (leaky_relu): {leaky_relu_test_accuracy * 100:.2f}%")

def cnn_with_swish(input_shape=(32, 32, 3), num_classes=10):
    model = models.Sequential()
    model.add(layers.Input(shape=input_shape))
    model.add(layers.Conv2D(32, (3, 3), padding='same'))
    model.add(layers.Activation(tf.keras.activations.swish))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Conv2D(64, (3, 3), padding='same'))
    model.add(layers.Activation(tf.keras.activations.swish))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Conv2D(128, (3, 3), padding='same'))
    model.add(layers.Activation(tf.keras.activations.swish))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Flatten())
    model.add(layers.Dense(256))
    model.add(layers.Activation(tf.keras.activations.swish))
    model.add(layers.Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

swish_model = cnn_with_swish()
swish_history = swish_model.fit(x_train_final, y_train_final,
                                  epochs=25,
                                  batch_size=64,
                                  validation_data=(x_test_final, y_test_final))

swish_test_loss, swish_test_accuracy = swish_model.evaluate(x_test_final, y_test_final)
print(f"Accuracy (swish): {swish_test_accuracy * 100:.2f}%")

def cnn_with_relu(input_shape=(32, 32, 3), num_classes=10):
    model = models.Sequential()
    model.add(layers.Input(shape=input_shape))
    model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu'))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Flatten())
    model.add(layers.Dense(256, activation='relu'))
    model.add(layers.Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

relu_model = cnn_with_relu()
relu_history = relu_model.fit(x_train_final, y_train_final,
                                  epochs=25,
                                  batch_size=64,
                                  validation_data=(x_test_final, y_test_final))

relu_test_loss, relu_test_accuracy = relu_model.evaluate(x_test_final, y_test_final)
print(f"Accuracy (relu): {relu_test_accuracy * 100:.2f}%")

def cnn_with_swish_and_relu(input_shape=(32, 32, 3), num_classes=10):
    model = models.Sequential()
    model.add(layers.Input(shape=input_shape))
    model.add(layers.Conv2D(32, (3, 3), padding='same'))
    model.add(layers.Conv2D(32, (3, 3), padding='same'))
    model.add(layers.Activation(tf.keras.activations.swish))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Dropout(0.2))
    model.add(layers.Conv2D(64, (3, 3), padding='same'))
    model.add(layers.Conv2D(64, (3, 3), padding='same'))
    model.add(layers.Activation(tf.keras.activations.swish))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Dropout(0.3))
    model.add(layers.Conv2D(128, (3, 3), padding='same'))
    model.add(layers.Conv2D(128, (3, 3), padding='same'))
    model.add(layers.Activation(tf.keras.activations.swish))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Dropout(0.4))
    model.add(layers.Flatten())
    model.add(layers.Dense(256, activation='relu'))
    model.add(layers.Dense(256, activation='relu'))
    model.add(layers.Dropout(0.4))
    model.add(layers.Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

swish_and_relu_model = cnn_with_swish_and_relu()
swish_and_relu_history = swish_and_relu_model.fit(x_train_final, y_train_final,
                                  epochs=25,
                                  batch_size=64,
                                  validation_data=(x_test_final, y_test_final))

swish_and_relu_test_loss, swish_and_relu_test_accuracy = swish_and_relu_model.evaluate(x_test_final, y_test_final)
print(f"Accuracy (swish_and_relu): {swish_and_relu_test_accuracy * 100:.2f}%")

def cnn_with_swish_and_leaky_relu(input_shape=(32, 32, 3), num_classes=10):
    model = models.Sequential()
    model.add(layers.Input(shape=input_shape))
    model.add(layers.Conv2D(32, (3, 3), padding='same'))
    model.add(layers.Conv2D(32, (3, 3), padding='same'))
    model.add(layers.Activation(tf.keras.activations.swish))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Dropout(0.2))
    model.add(layers.Conv2D(64, (3, 3), padding='same'))
    model.add(layers.Conv2D(64, (3, 3), padding='same'))
    model.add(layers.Activation(tf.keras.activations.swish))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Dropout(0.2))
    model.add(layers.Conv2D(128, (3, 3), padding='same'))
    model.add(layers.Conv2D(128, (3, 3), padding='same'))
    model.add(layers.Activation(tf.keras.activations.swish))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Dropout(0.3))
    model.add(layers.Flatten())
    model.add(layers.Dense(256))
    model.add(LeakyReLU(alpha=0.1))
    model.add(layers.Dropout(0.3))
    model.add(layers.Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

swish_and_leaky_relu_model = cnn_with_swish_and_leaky_relu()
swish_and_leaky_relu_history = swish_and_leaky_relu_model.fit(x_train_final, y_train_final,
                                  epochs=25,
                                  batch_size=64,
                                  validation_data=(x_test_final, y_test_final))

swish_and_leaky_relu_test_loss, swish_and_leaky_relu_test_accuracy = swish_and_leaky_relu_model.evaluate(x_test_final, y_test_final)
print(f"Accuracy (swish_and_leaky_relu): {swish_and_leaky_relu_test_accuracy * 100:.2f}%")

"""## 4. Using various optimizers and learning rate"""

def build_model():
    model = models.Sequential()
    model.add(layers.Input(shape=(32, 32, 3)))
    model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu'))
    model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu'))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Dropout(0.2))
    model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))
    model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Dropout(0.2))
    model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))
    model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Dropout(0.2))
    model.add(layers.Flatten())
    model.add(layers.Dense(256, activation='relu'))
    model.add(layers.Dropout(0.3))
    model.add(layers.Dense(10, activation='softmax'))
    return model

optimizers = {
    'Adam': Adam,
    'SGD': SGD,
    'RMSprop': RMSprop,
    'Adagrad': Adagrad
}
learning_rates = [0.01, 0.001, 0.0001]

results = []
for opt_name, opt_class in optimizers.items():
    for lr in learning_rates:
        print(f"\nTesting {opt_name} with learning rate {lr}")
        model = build_model()
        optimizer = opt_class(learning_rate=lr)
        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

        history = model.fit(
            x_train_final, y_train_final,
            epochs=25,
            batch_size=64,
            validation_data=(x_test_final, y_test_final)
        )

        test_loss, test_acc = model.evaluate(x_test_final, y_test_final, verbose=0)
        print(f"Accuracy: {test_acc * 100:.2f}%")
        results.append((opt_name, lr, test_acc * 100))

print("Summary of results:")
for opt_name, lr, acc in results:
    print(f"{opt_name} (lr={lr}) → Accuracy: {acc:.2f}%")

df = pd.DataFrame(results, columns=["Optimizer", "Learning Rate", "Accuracy"])
sns.set(style="whitegrid")
plt.figure(figsize=(10, 6))
sns.lineplot(data=df, x="Learning Rate", y="Accuracy", hue="Optimizer", marker="o")
plt.title("Test Accuracy vs. Learning Rate for Different Optimizers", fontsize=14)
plt.xlabel("Learning Rate")
plt.ylabel("Test Accuracy (%)")
plt.legend(title="Optimizer")
plt.xscale("log")
plt.tight_layout()
plt.show()

"""## 5. Using various batch sizes and epochs for training"""

batch_sizes = [32, 64, 128]
epochs_list = [10, 20, 30]
output = []

for batch_size in batch_sizes:
    for num_epochs in epochs_list:
        print(f"\nTraining with batch size {batch_size} and epochs {num_epochs}")

        model = build_model()
        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

        history = model.fit(
            x_train_final, y_train_final,
            epochs=num_epochs,
            batch_size=batch_size,
            validation_data=(x_test_final, y_test_final)
        )

        test_loss, test_acc = model.evaluate(x_test_final, y_test_final, verbose=0)
        print(f"Accuracy: {test_acc * 100:.2f}%")
        output.append((batch_size, num_epochs, test_acc * 100))

print("Summary of batch size and epoch experiments:")
for batch_size, num_epochs, acc in output:
    print(f"Batch Size: {batch_size}, Epochs: {num_epochs} → Accuracy: {acc:.2f}%")

df2 = pd.DataFrame(output, columns=["Batch Size", "Epochs", "Accuracy"])
sns.set(style="whitegrid")
plt.figure(figsize=(10, 6))
sns.lineplot(data=df2, x="Epochs", y="Accuracy", hue="Batch Size", marker="o")
plt.title("Test Accuracy vs. Epochs for Different Batch Sizes", fontsize=14)
plt.xlabel("Epochs")
plt.ylabel("Test Accuracy (%)")
plt.legend(title="Batch Size")
plt.tight_layout()
plt.show()

"""---------------------------------------------
# Repeating the above experiment in (5) using a different CIFAR-100 subset
---------------------------------------------

# Data Loading, Preparation, and Preprocessing
"""

(x_train_2, y_train_2), (x_test_2, y_test_2) = tf.keras.datasets.cifar100.load_data()
CIFAR100_LABELS = [
    'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle', 'bowl', 'boy',
    'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock',
    'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 'dolphin', 'elephant', 'flatfish',
    'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',
    'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', 'oak_tree', 'orange',
    'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',
    'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark', 'shrew', 'skunk',
    'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table',
    'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe',
    'whale', 'willow_tree', 'wolf', 'woman', 'worm'
]

np.random.seed(44)  # To ensure reproducibility
#selected_classes_2 = np.random.choice(CIFAR100_LABELS, 10, replace=False)
selected_classes_2 = ['orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',]
print(f"Selected classes (another): {selected_classes_2}")

selected_class_indices_2 = map_classes_to_indices(selected_classes_2, CIFAR100_LABELS)

x_train_selected_2, y_train_selected_2 = extract_data_for_classes(x_train_2, y_train_2, selected_class_indices_2)
x_test_selected_2, y_test_selected_2 = extract_data_for_classes(x_test_2, y_test_2, selected_class_indices_2)

y_train_selected_2 = remap_labels(y_train_selected_2, selected_class_indices_2)
y_test_selected_2 = remap_labels(y_test_selected_2, selected_class_indices_2)

x_train_final_2, y_train_final_2 = reduce_dataset_size(x_train_selected_2, y_train_selected_2, 300)
x_test_final_2, y_test_final_2 = reduce_dataset_size(x_test_selected_2, y_test_selected_2, 50)

x_train_final_2, y_train_final_2 = preprocess_data(x_train_final_2, y_train_final_2, len(selected_classes_2))
x_test_final_2, y_test_final_2 = preprocess_data(x_test_final_2, y_test_final_2, len(selected_classes_2))

print(f"Train Data 2 Shape: {x_train_final_2.shape}, Train 2 Labels: {y_train_final_2.shape}")
print(f"Test Data 2 Shape: {x_test_final_2.shape}, Test 2 Labels: {y_test_final_2.shape}")

"""# Evaluating CNN model performance on Imagenet dataset"""

cnn_model_2 = build_cnn_model()
cnn_model_2.summary()

history_2 = cnn_model_2.fit(x_train_final_2, y_train_final_2,
                    epochs=25,
                    batch_size=64,
                    validation_data=(x_test_final_2, y_test_final_2))

test_loss_2, test_accuracy_2 = cnn_model_2.evaluate(x_test_final_2, y_test_final_2)
print(f"Accuracy 2: {test_accuracy_2 * 100:.2f}%")

plt.plot(history_2.history['accuracy'], label='Train Accuracy')
plt.plot(history_2.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training vs Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

print("")
plt.plot(history_2.history['loss'], label='Train Loss')
plt.plot(history_2.history['val_loss'], label='Validation Loss')
plt.title('Training vs Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

"""# Experiments to Improve Accuracy

## 1. Increasing the size and depth of the inner layers
"""

deeper_model_2 = build_deeper_model()
deeper_history_2 = deeper_model_2.fit(x_train_final_2, y_train_final_2, epochs=25, batch_size=64, validation_data=(x_test_final_2, y_test_final_2))

deeper_test_loss_2, deeper_test_accuracy_2 = deeper_model_2.evaluate(x_test_final_2, y_test_final_2)
print(f"Accuracy 2 after optimizations: {deeper_test_accuracy_2 * 100:.2f}%")

"""## 2. Using fewer or more convolutional/maxpooling layers and different shapes"""

simpler_model_2 = build_simpler_model()
history_simple_2 = simpler_model_2.fit(x_train_final_2, y_train_final_2,
                                  epochs=25,
                                  batch_size=64,
                                  validation_data=(x_test_final_2, y_test_final_2))

simpler_test_loss_2, simpler_test_accuracy_2 = simpler_model_2.evaluate(x_test_final_2, y_test_final_2)
print(f"Accuracy (simpler_2): {simpler_test_accuracy_2 * 100:.2f}%")

deep_model_2 = build_deep_model()
history_deep_2 = deep_model_2.fit(x_train_final_2, y_train_final_2,
                                  epochs=25,
                                  batch_size=64,
                                  validation_data=(x_test_final_2, y_test_final_2))

deep_test_loss_2, deep_test_accuracy_2 = deep_model_2.evaluate(x_test_final_2, y_test_final_2)
print(f"Accuracy (deep_2): {deep_test_accuracy_2 * 100:.2f}%")

"""## 3. Using different activation functions in the inner layers and in the convolutional layers"""

tanh_model_2 = cnn_with_tanh()
tanh_history_2 = tanh_model_2.fit(x_train_final_2, y_train_final_2,
                                  epochs=25,
                                  batch_size=64,
                                  validation_data=(x_test_final_2, y_test_final_2))

tanh_test_loss_2, tanh_test_accuracy_2 = tanh_model_2.evaluate(x_test_final_2, y_test_final_2)
print(f"Accuracy (tanh_2): {tanh_test_accuracy_2 * 100:.2f}%")

sigmoid_model_2 = cnn_with_sigmoid()
sigmoid_history_2 = sigmoid_model_2.fit(x_train_final_2, y_train_final_2,
                                  epochs=25,
                                  batch_size=64,
                                  validation_data=(x_test_final_2, y_test_final_2))

sigmoid_test_loss_2, sigmoid_test_accuracy_2 = sigmoid_model_2.evaluate(x_test_final_2, y_test_final_2)
print(f"Accuracy (sigmoid_2): {sigmoid_test_accuracy_2 * 100:.2f}%")

leaky_relu_model_2 = cnn_with_leaky_relu()
leaky_relu_history_2 = leaky_relu_model_2.fit(x_train_final_2, y_train_final_2,
                                  epochs=25,
                                  batch_size=64,
                                  validation_data=(x_test_final_2, y_test_final_2))

leaky_relu_test_loss_2, leaky_relu_test_accuracy_2 = leaky_relu_model_2.evaluate(x_test_final_2, y_test_final_2)
print(f"Accuracy (leaky_relu_2): {leaky_relu_test_accuracy_2 * 100:.2f}%")

swish_model_2 = cnn_with_swish()
swish_history_2 = swish_model_2.fit(x_train_final_2, y_train_final_2,
                                  epochs=25,
                                  batch_size=64,
                                  validation_data=(x_test_final_2, y_test_final_2))

swish_test_loss_2, swish_test_accuracy_2 = swish_model_2.evaluate(x_test_final_2, y_test_final_2)
print(f"Accuracy (swish_2): {swish_test_accuracy_2 * 100:.2f}%")

relu_model_2 = cnn_with_relu()
relu_history_2 = relu_model_2.fit(x_train_final_2, y_train_final_2,
                                  epochs=25,
                                  batch_size=64,
                                  validation_data=(x_test_final_2, y_test_final_2))

relu_test_loss_2, relu_test_accuracy_2 = relu_model_2.evaluate(x_test_final_2, y_test_final_2)
print(f"Accuracy (relu_2): {relu_test_accuracy_2 * 100:.2f}%")

swish_and_relu_model_2 = cnn_with_swish_and_relu()
swish_and_relu_history_2 = swish_and_relu_model_2.fit(x_train_final_2, y_train_final_2,
                                  epochs=25,
                                  batch_size=64,
                                  validation_data=(x_test_final_2, y_test_final_2))

swish_and_relu_test_loss_2, swish_and_relu_test_accuracy_2 = swish_and_relu_model_2.evaluate(x_test_final_2, y_test_final_2)
print(f"Accuracy (swish_and_relu_2): {swish_and_relu_test_accuracy_2 * 100:.2f}%")

swish_and_leaky_relu_model_2 = cnn_with_swish_and_leaky_relu()
swish_and_leaky_relu_history_2 = swish_and_leaky_relu_model_2.fit(x_train_final_2, y_train_final_2,
                                  epochs=25,
                                  batch_size=64,
                                  validation_data=(x_test_final_2, y_test_final_2))

swish_and_leaky_relu_test_loss_2, swish_and_leaky_relu_test_accuracy_2 = swish_and_leaky_relu_model_2.evaluate(x_test_final_2, y_test_final_2)
print(f"Accuracy (swish_and_leaky_relu_2): {swish_and_leaky_relu_test_accuracy_2 * 100:.2f}%")

"""## 4. Using various optimizers and learning rate"""

results_2 = []
for opt_name, opt_class in optimizers.items():
    for lr in learning_rates:
        print(f"\nTesting {opt_name} with learning rate {lr}")
        model = build_model()
        optimizer = opt_class(learning_rate=lr)
        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

        history = model.fit(
            x_train_final_2, y_train_final_2,
            epochs=25,
            batch_size=64,
            validation_data=(x_test_final_2, y_test_final_2)
        )

        test_loss, test_acc = model.evaluate(x_test_final_2, y_test_final_2, verbose=0)
        print(f"Accuracy: {test_acc * 100:.2f}%")
        results_2.append((opt_name, lr, test_acc * 100))

print("Summary of results:")
for opt_name, lr, acc in results_2:
    print(f"{opt_name} (lr={lr}) → Accuracy: {acc:.2f}%")

df_2 = pd.DataFrame(results_2, columns=["Optimizer", "Learning Rate", "Accuracy"])
sns.set(style="whitegrid")
plt.figure(figsize=(10, 6))
sns.lineplot(data=df_2, x="Learning Rate", y="Accuracy", hue="Optimizer", marker="o")
plt.title("Test Accuracy vs. Learning Rate for Different Optimizers", fontsize=14)
plt.xlabel("Learning Rate")
plt.ylabel("Test Accuracy (%)")
plt.legend(title="Optimizer")
plt.xscale("log")
plt.tight_layout()
plt.show()

"""## 5. Using various batch sizes and epochs for training"""

output_2 = []

for batch_size in batch_sizes:
    for num_epochs in epochs_list:
        print(f"\nTraining with batch size {batch_size} and epochs {num_epochs}")

        model = build_model()
        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

        history = model.fit(
            x_train_final_2, y_train_final_2,
            epochs=num_epochs,
            batch_size=batch_size,
            validation_data=(x_test_final_2, y_test_final_2)
        )

        test_loss, test_acc = model.evaluate(x_test_final_2, y_test_final_2, verbose=0)
        print(f"Accuracy: {test_acc * 100:.2f}%")
        output_2.append((batch_size, num_epochs, test_acc * 100))

print("Summary of batch size and epoch experiments:")
for batch_size, num_epochs, acc in output_2:
    print(f"Batch Size: {batch_size}, Epochs: {num_epochs} → Accuracy: {acc:.2f}%")

df_batch_2 = pd.DataFrame(output_2, columns=["Batch Size", "Epochs", "Accuracy"])
sns.set(style="whitegrid")
plt.figure(figsize=(10, 6))
sns.lineplot(data=df_batch_2, x="Epochs", y="Accuracy", hue="Batch Size", marker="o")
plt.title("Test Accuracy vs. Epochs for Different Batch Sizes", fontsize=14)
plt.xlabel("Epochs")
plt.ylabel("Test Accuracy (%)")
plt.legend(title="Batch Size")
plt.tight_layout()
plt.show()

"""--------------------------------------------
# Repeating the above experiment in (5) using a subset of the imagenet data set (8-12 classes, a random subset from each class of suitable size)
--------------------------------------------

# Data Loading, Preparation, and Preprocessing
"""

dataset_name = "imagenette/320px"
(train_ds, val_ds), ds_info = tfds.load(dataset_name, split=['train', 'validation'], as_supervised=True, with_info=True)
class_names = ds_info.features['label'].names

# Selecting a random subset of classes
np.random.seed(42)
selected_class_indices = np.random.choice(range(len(class_names)), 10, replace=False)
selected_class_names = [class_names[i] for i in selected_class_indices]

def filter_and_remap(dataset, selected_indices, samples_per_class, image_size=(32, 32)):
    class_counts = {i: 0 for i in range(len(selected_indices))}
    selected_images = []
    selected_labels = []

    for image, label in tfds.as_numpy(dataset):
        if label in selected_indices:
            new_label = selected_indices.tolist().index(label)

            if class_counts[new_label] < samples_per_class:
                image = tf.image.resize(image, image_size).numpy().astype('float32') / 255.0
                selected_images.append(image)
                selected_labels.append(new_label)
                class_counts[new_label] += 1

        if all(v >= samples_per_class for v in class_counts.values()):
            break

    return np.array(selected_images), np.array(selected_labels)

x_train_3, y_train_3 = filter_and_remap(train_ds, selected_class_indices, samples_per_class=300)
x_val_3, y_val_3 = filter_and_remap(val_ds, selected_class_indices, samples_per_class=50)

y_train_3 = to_categorical(y_train_3, num_classes=10)
y_val_3 = to_categorical(y_val_3, num_classes=10)

print(f"Train shape: {x_train_3.shape}, Labels: {y_train_3.shape}")
print(f"Val shape: {x_val_3.shape}, Labels: {y_val_3.shape}")

"""# Evaluating CNN model performance on Imagenet dataset"""

cnn_model_3 = build_cnn_model()
cnn_model_3.summary()

history_3 = cnn_model_3.fit(
    x_train_3, y_train_3,
    epochs=25,
    batch_size=64,
    validation_data=(x_val_3, y_val_3)
)

test_loss_3, test_accuracy_3 = cnn_model_3.evaluate(x_val_3, y_val_3)
print(f"Accuracy: {test_accuracy_3 * 100:.2f}%")

plt.plot(history_3.history['accuracy'], label='Train Accuracy')
plt.plot(history_3.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training vs Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

print("")
plt.plot(history_3.history['loss'], label='Train Loss')
plt.plot(history_3.history['val_loss'], label='Validation Loss')
plt.title('Training vs Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

"""# Experiments to improve accuracy

## 1. Increasing the size and depth of the inner layers
"""

deeper_model_3 = build_deeper_model()
deeper_history_3 = deeper_model_3.fit(x_train_3, y_train_3, epochs=25, batch_size=64, validation_data=(x_val_3, y_val_3))

deeper_test_loss_3, deeper_test_accuracy_3 = deeper_model_3.evaluate(x_val_3, y_val_3)
print(f"Accuracy after optimizations: {deeper_test_accuracy_3 * 100:.2f}%")

"""## 2. Using fewer or more convolutional/maxpooling layers and different shapes"""

simpler_model_3 = build_simpler_model()
history_simple_3 = simpler_model_3.fit(x_train_3, y_train_3,
                                  epochs=25,
                                  batch_size=64,
                                  validation_data=(x_val_3, y_val_3))

simpler_test_loss_3, simpler_test_accuracy_3 = simpler_model_3.evaluate(x_val_3, y_val_3)
print(f"Accuracy (simpler_3): {simpler_test_accuracy_3 * 100:.2f}%")

deep_model_3 = build_deep_model()
history_deep_3 = deep_model_3.fit(x_train_3, y_train_3,
                                  epochs=25,
                                  batch_size=64,
                                  validation_data=(x_val_3, y_val_3))

deep_test_loss_3, deep_test_accuracy_3 = deep_model_3.evaluate(x_val_3, y_val_3)
print(f"Accuracy (deep_3): {deep_test_accuracy_3 * 100:.2f}%")

"""## 3. Using different activation functions in the inner layers and in the convolutional layers"""

tanh_model_3 = cnn_with_tanh()
tanh_history_3 = tanh_model_3.fit(x_train_3, y_train_3,
                                  epochs=25,
                                  batch_size=64,
                                  validation_data=(x_val_3, y_val_3))

tanh_test_loss_3, tanh_test_accuracy_3 = tanh_model_3.evaluate(x_val_3, y_val_3)
print(f"Accuracy (tanh_3): {tanh_test_accuracy_3 * 100:.2f}%")

sigmoid_model_3 = cnn_with_sigmoid()
sigmoid_history_3 = sigmoid_model_3.fit(x_train_3, y_train_3,
                                  epochs=25,
                                  batch_size=64,
                                  validation_data=(x_val_3, y_val_3))

sigmoid_test_loss_3, sigmoid_test_accuracy_3 = sigmoid_model_3.evaluate(x_val_3, y_val_3)
print(f"Accuracy (sigmoid_3): {sigmoid_test_accuracy_3 * 100:.2f}%")

leaky_relu_model_3 = cnn_with_leaky_relu()
leaky_relu_history_3 = leaky_relu_model_3.fit(x_train_3, y_train_3,
                                  epochs=25,
                                  batch_size=64,
                                  validation_data=(x_val_3, y_val_3))

leaky_relu_test_loss_3, leaky_relu_test_accuracy_3 = leaky_relu_model_3.evaluate(x_val_3, y_val_3)
print(f"Accuracy (leaky_relu_3): {leaky_relu_test_accuracy_3 * 100:.2f}%")

swish_model_3 = cnn_with_swish()
swish_history_3 = swish_model_3.fit(x_train_3, y_train_3,
                                  epochs=25,
                                  batch_size=64,
                                  validation_data=(x_val_3, y_val_3))

swish_test_loss_3, swish_test_accuracy_3 = swish_model_3.evaluate(x_val_3, y_val_3)
print(f"Accuracy (swish_3): {swish_test_accuracy_3 * 100:.2f}%")

relu_model_3 = cnn_with_relu()
relu_history_3 = relu_model_3.fit(x_train_3, y_train_3,
                                  epochs=25,
                                  batch_size=64,
                                  validation_data=(x_val_3, y_val_3))

relu_test_loss_3, relu_test_accuracy_3 = relu_model_3.evaluate(x_val_3, y_val_3)
print(f"Accuracy (relu_3): {relu_test_accuracy_3 * 100:.2f}%")

swish_and_relu_model_3 = cnn_with_swish_and_relu()
swish_and_relu_history_3 = swish_and_relu_model_3.fit(x_train_3, y_train_3,
                                  epochs=25,
                                  batch_size=64,
                                  validation_data=(x_val_3, y_val_3))

swish_and_relu_test_loss_3, swish_and_relu_test_accuracy_3 = swish_and_relu_model_3.evaluate(x_val_3, y_val_3)
print(f"Accuracy (swish_and_relu_3): {swish_and_relu_test_accuracy_3 * 100:.2f}%")

swish_and_leaky_relu_model_3 = cnn_with_swish_and_leaky_relu()
swish_and_leaky_relu_history_3 = swish_and_leaky_relu_model_3.fit(x_train_3, y_train_3,
                                  epochs=25,
                                  batch_size=64,
                                  validation_data=(x_val_3, y_val_3))

swish_and_leaky_relu_test_loss_3, swish_and_leaky_relu_test_accuracy_3 = swish_and_leaky_relu_model_3.evaluate(x_val_3, y_val_3)
print(f"Accuracy (swish_and_leaky_relu_3): {swish_and_leaky_relu_test_accuracy_3 * 100:.2f}%")

"""## 4. Using various optimizers and learning rate"""

results_3 = []
for opt_name, opt_class in optimizers.items():
    for lr in learning_rates:
        print(f"\nTesting {opt_name} with learning rate {lr}")
        model = build_model()
        optimizer = opt_class(learning_rate=lr)
        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

        history = model.fit(
            x_train_final_2, y_train_final_2,
            epochs=25,
            batch_size=64,
            validation_data=(x_test_final_2, y_test_final_2)
        )

        test_loss, test_acc = model.evaluate(x_test_final_2, y_test_final_2, verbose=0)
        print(f"Accuracy: {test_acc * 100:.2f}%")
        results_3.append((opt_name, lr, test_acc * 100))

print("Summary of results:")
for opt_name, lr, acc in results_3:
    print(f"{opt_name} (lr={lr}) → Accuracy: {acc:.2f}%")

df_3 = pd.DataFrame(results_3, columns=["Optimizer", "Learning Rate", "Accuracy"])
sns.set(style="whitegrid")
plt.figure(figsize=(10, 6))
sns.lineplot(data=df_3, x="Learning Rate", y="Accuracy", hue="Optimizer", marker="o")
plt.title("Test Accuracy vs. Learning Rate for Different Optimizers", fontsize=14)
plt.xlabel("Learning Rate")
plt.ylabel("Test Accuracy (%)")
plt.legend(title="Optimizer")
plt.xscale("log")
plt.tight_layout()
plt.show()

"""## 5. Using various batch sizes and epochs for training"""

output_3 = []

for batch_size in batch_sizes:
    for num_epochs in epochs_list:
        print(f"\nTraining with batch size {batch_size} and epochs {num_epochs}")
        model = build_model()
        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

        history = model.fit(
            x_train_final_2, y_train_final_2,
            epochs=num_epochs,
            batch_size=batch_size,
            validation_data=(x_test_final_2, y_test_final_2)
        )

        test_loss, test_acc = model.evaluate(x_test_final_2, y_test_final_2, verbose=0)
        print(f"Accuracy: {test_acc * 100:.2f}%")
        output_3.append((batch_size, num_epochs, test_acc * 100))

print("Summary of batch size and epoch experiments:")
for batch_size, num_epochs, acc in output_3:
    print(f"Batch Size: {batch_size}, Epochs: {num_epochs} → Accuracy: {acc:.2f}%")

df_batch_3 = pd.DataFrame(output_3, columns=["Batch Size", "Epochs", "Accuracy"])
sns.set(style="whitegrid")
plt.figure(figsize=(10, 6))
sns.lineplot(data=df_batch_3, x="Epochs", y="Accuracy", hue="Batch Size", marker="o")
plt.title("Test Accuracy vs. Epochs for Different Batch Sizes", fontsize=14)
plt.xlabel("Epochs")
plt.ylabel("Test Accuracy (%)")
plt.legend(title="Batch Size")
plt.tight_layout()
plt.show()

